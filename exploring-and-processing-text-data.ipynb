{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191e4555",
   "metadata": {
    "papermill": {
     "duration": 0.038181,
     "end_time": "2022-05-18T13:42:35.628626",
     "exception": false,
     "start_time": "2022-05-18T13:42:35.590445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Converting Text Data to Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f69f1",
   "metadata": {
    "papermill": {
     "duration": 0.035188,
     "end_time": "2022-05-18T13:42:35.699533",
     "exception": false,
     "start_time": "2022-05-18T13:42:35.664345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  I'm going to discuss the following recipes under text preprocessing and exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66334464",
   "metadata": {
    "papermill": {
     "duration": 0.042544,
     "end_time": "2022-05-18T13:42:35.777670",
     "exception": false,
     "start_time": "2022-05-18T13:42:35.735126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recipe 1. Lowercasing\n",
    "Recipe 2. Punctuation removal\n",
    "Recipe 3. Stop words removal\n",
    "Recipe 4. Text standardization\n",
    "Recipe 5. Spelling correction\n",
    "Recipe 6. Tokenization\n",
    "Recipe 7. Stemming\n",
    "Recipe 8. Lemmatization\n",
    "Recipe 9. Exploratory data analysis\n",
    "Recipe 10. End-to-end processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a044247",
   "metadata": {
    "papermill": {
     "duration": 0.038916,
     "end_time": "2022-05-18T13:42:35.862126",
     "exception": false,
     "start_time": "2022-05-18T13:42:35.823210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Let‚Äôs create a list of strings and assign it to a variable.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4564fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:35.935582Z",
     "iopub.status.busy": "2022-05-18T13:42:35.934849Z",
     "iopub.status.idle": "2022-05-18T13:42:35.946693Z",
     "shell.execute_reply": "2022-05-18T13:42:35.946144Z"
    },
    "papermill": {
     "duration": 0.05107,
     "end_time": "2022-05-18T13:42:35.948778",
     "exception": false,
     "start_time": "2022-05-18T13:42:35.897708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ['I am annie', 'I read in 12',\n",
    "        'I love NLP', 'I learning NLP in 2 weeks', \n",
    "        'python is the best', 'R is good langauage', \n",
    "        'I like this book','I wanna more books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970a51f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.023145Z",
     "iopub.status.busy": "2022-05-18T13:42:36.022582Z",
     "iopub.status.idle": "2022-05-18T13:42:36.035616Z",
     "shell.execute_reply": "2022-05-18T13:42:36.034783Z"
    },
    "papermill": {
     "duration": 0.052627,
     "end_time": "2022-05-18T13:42:36.037641",
     "exception": false,
     "start_time": "2022-05-18T13:42:35.985014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       tweet\n",
      "0                 I am annie\n",
      "1               I read in 12\n",
      "2                 I love NLP\n",
      "3  I learning NLP in 2 weeks\n",
      "4         python is the best\n",
      "5        R is good langauage\n",
      "6           I like this book\n",
      "7         I wanna more books\n"
     ]
    }
   ],
   "source": [
    "#convert list to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet':text})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd3a47c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.113040Z",
     "iopub.status.busy": "2022-05-18T13:42:36.112522Z",
     "iopub.status.idle": "2022-05-18T13:42:36.125969Z",
     "shell.execute_reply": "2022-05-18T13:42:36.125131Z"
    },
    "papermill": {
     "duration": 0.052872,
     "end_time": "2022-05-18T13:42:36.128005",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.075133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   i am annie\n",
       "1                 i read in 12\n",
       "2                   i love nlp\n",
       "3    i learning nlp in 2 weeks\n",
       "4           python is the best\n",
       "5          r is good langauage\n",
       "6             i like this book\n",
       "7           i wanna more books\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower()\n",
    "for x in x.split()))\n",
    "df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5e914a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.209737Z",
     "iopub.status.busy": "2022-05-18T13:42:36.209216Z",
     "iopub.status.idle": "2022-05-18T13:42:36.213746Z",
     "shell.execute_reply": "2022-05-18T13:42:36.213009Z"
    },
    "papermill": {
     "duration": 0.051313,
     "end_time": "2022-05-18T13:42:36.215733",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.164420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text2 = ['I am annie', 'I read in 12',\n",
    "        'I love NLP', 'I learning NLP in 2 weeks', \n",
    "        'python is the best', 'R is good langauage', \n",
    "        'I like this book','I wanna more books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6bc4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.308068Z",
     "iopub.status.busy": "2022-05-18T13:42:36.307500Z",
     "iopub.status.idle": "2022-05-18T13:42:36.321910Z",
     "shell.execute_reply": "2022-05-18T13:42:36.321145Z"
    },
    "papermill": {
     "duration": 0.057938,
     "end_time": "2022-05-18T13:42:36.323919",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.265981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet2</th>\n",
       "      <th>tweet_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am annie</td>\n",
       "      <td>i am annie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I read in 12</td>\n",
       "      <td>i read in 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love NLP</td>\n",
       "      <td>i love nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I learning NLP in 2 weeks</td>\n",
       "      <td>i learning nlp in 2 weeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>python is the best</td>\n",
       "      <td>python is the best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R is good langauage</td>\n",
       "      <td>r is good langauage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I like this book</td>\n",
       "      <td>i like this book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I wanna more books</td>\n",
       "      <td>i wanna more books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tweet2                tweet_lower\n",
       "0                 I am annie                 i am annie\n",
       "1               I read in 12               i read in 12\n",
       "2                 I love NLP                 i love nlp\n",
       "3  I learning NLP in 2 weeks  i learning nlp in 2 weeks\n",
       "4         python is the best         python is the best\n",
       "5        R is good langauage        r is good langauage\n",
       "6           I like this book           i like this book\n",
       "7         I wanna more books         i wanna more books"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'tweet2': text2})\n",
    "df['tweet_lower'] = df['tweet2'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72eac857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.400867Z",
     "iopub.status.busy": "2022-05-18T13:42:36.400326Z",
     "iopub.status.idle": "2022-05-18T13:42:36.406441Z",
     "shell.execute_reply": "2022-05-18T13:42:36.405664Z"
    },
    "papermill": {
     "duration": 0.04697,
     "end_time": "2022-05-18T13:42:36.408342",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.361372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in publishing and graphic design, lorem ipsum is a placeholder text commonly used to demonstrate'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'In publishing and graphic design, Lorem ipsum is a placeholder text commonly used to demonstrate'\n",
    "\n",
    "df2= text2.lower()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2daf3",
   "metadata": {
    "papermill": {
     "duration": 0.036953,
     "end_time": "2022-05-18T13:42:36.482365",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.445412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a4c4f",
   "metadata": {
    "papermill": {
     "duration": 0.037088,
     "end_time": "2022-05-18T13:42:36.556834",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.519746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this recipeüòâ, I'm going to discuss how to remove punctuation from the\n",
    "text data. This step is very important as punctuation doesn‚Äôt add any extra\n",
    "information or value. Hence removal of all such instances will help reduce\n",
    "the size of the data and increase computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6552d669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.633920Z",
     "iopub.status.busy": "2022-05-18T13:42:36.633509Z",
     "iopub.status.idle": "2022-05-18T13:42:36.639058Z",
     "shell.execute_reply": "2022-05-18T13:42:36.637471Z"
    },
    "papermill": {
     "duration": 0.048334,
     "end_time": "2022-05-18T13:42:36.642212",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.593878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ['I am annie.', 'I read in 12!',\n",
    "        'I love NLP$', 'I learning NLP in 2 weeks*', \n",
    "        'python is the best/', 'R is good langauage@', \n",
    "        'I like this book!','I wanna more books.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a2abe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.723976Z",
     "iopub.status.busy": "2022-05-18T13:42:36.723546Z",
     "iopub.status.idle": "2022-05-18T13:42:36.730648Z",
     "shell.execute_reply": "2022-05-18T13:42:36.730107Z"
    },
    "papermill": {
     "duration": 0.050963,
     "end_time": "2022-05-18T13:42:36.733629",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.682666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        tweet\n",
      "0                 I am annie.\n",
      "1               I read in 12!\n",
      "2                 I love NLP$\n",
      "3  I learning NLP in 2 weeks*\n",
      "4         python is the best/\n",
      "5        R is good langauage@\n",
      "6           I like this book!\n",
      "7         I wanna more books.\n"
     ]
    }
   ],
   "source": [
    "#convert list to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet':text})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b02e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:36.814105Z",
     "iopub.status.busy": "2022-05-18T13:42:36.813590Z",
     "iopub.status.idle": "2022-05-18T13:42:36.821543Z",
     "shell.execute_reply": "2022-05-18T13:42:36.820476Z"
    },
    "papermill": {
     "duration": 0.050051,
     "end_time": "2022-05-18T13:42:36.824663",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.774612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                   I am annie\n",
       "1                 I read in 12\n",
       "2                   I love NLP\n",
       "3    I learning NLP in 2 weeks\n",
       "4           python is the best\n",
       "5          R is good langauage\n",
       "6             I like this book\n",
       "7           I wanna more books\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "df['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c1188",
   "metadata": {
    "papermill": {
     "duration": 0.041809,
     "end_time": "2022-05-18T13:42:36.908484",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.866675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8835a30",
   "metadata": {
    "papermill": {
     "duration": 0.038758,
     "end_time": "2022-05-18T13:42:36.986242",
     "exception": false,
     "start_time": "2022-05-18T13:42:36.947484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Stop words are very common words that carry no meaning or less meaning compared\n",
    "to other keywords. If we remove the words that are less commonly used,\n",
    "we can focus on the important keywords instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f8f2c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:37.065366Z",
     "iopub.status.busy": "2022-05-18T13:42:37.065098Z",
     "iopub.status.idle": "2022-05-18T13:42:38.597201Z",
     "shell.execute_reply": "2022-05-18T13:42:38.596393Z"
    },
    "papermill": {
     "duration": 1.574562,
     "end_time": "2022-05-18T13:42:38.599668",
     "exception": false,
     "start_time": "2022-05-18T13:42:37.025106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11afe5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:38.678254Z",
     "iopub.status.busy": "2022-05-18T13:42:38.677968Z",
     "iopub.status.idle": "2022-05-18T13:42:38.682114Z",
     "shell.execute_reply": "2022-05-18T13:42:38.681523Z"
    },
    "papermill": {
     "duration": 0.045345,
     "end_time": "2022-05-18T13:42:38.683788",
     "exception": false,
     "start_time": "2022-05-18T13:42:38.638443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ['This is introduction to NLP', 'It is likely to be useful, to people',\n",
    "        'Machine learning is the new electrcity','There would be less hype around AI and more action going forward',\n",
    "        'python is the best tool', 'R is the good language','I like this Book',\n",
    "        'I want more books like this.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "940a768d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:38.763430Z",
     "iopub.status.busy": "2022-05-18T13:42:38.762855Z",
     "iopub.status.idle": "2022-05-18T13:42:38.771929Z",
     "shell.execute_reply": "2022-05-18T13:42:38.771353Z"
    },
    "papermill": {
     "duration": 0.051204,
     "end_time": "2022-05-18T13:42:38.773687",
     "exception": false,
     "start_time": "2022-05-18T13:42:38.722483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is introduction to NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is likely to be useful, to people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine learning is the new electrcity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There would be less hype around AI and more ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>python is the best tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R is the good language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I like this Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I want more books like this.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0                        This is introduction to NLP\n",
       "1               It is likely to be useful, to people\n",
       "2             Machine learning is the new electrcity\n",
       "3  There would be less hype around AI and more ac...\n",
       "4                            python is the best tool\n",
       "5                             R is the good language\n",
       "6                                   I like this Book\n",
       "7                       I want more books like this."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#onvert list to data frame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet':text})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586dfdc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:38.854210Z",
     "iopub.status.busy": "2022-05-18T13:42:38.853617Z",
     "iopub.status.idle": "2022-05-18T13:42:38.862688Z",
     "shell.execute_reply": "2022-05-18T13:42:38.861395Z"
    },
    "papermill": {
     "duration": 0.051452,
     "end_time": "2022-05-18T13:42:38.864645",
     "exception": false,
     "start_time": "2022-05-18T13:42:38.813193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0                        This is introduction to NLP   \n",
      "1               It is likely to be useful, to people   \n",
      "2             Machine learning is the new electrcity   \n",
      "3  There would be less hype around AI and more ac...   \n",
      "4                            python is the best tool   \n",
      "5                             R is the good language   \n",
      "6                                   I like this Book   \n",
      "7                       I want more books like this.   \n",
      "\n",
      "                             tweet_without_stopwords  \n",
      "0                              This introduction NLP  \n",
      "1                           It likely useful, people  \n",
      "2                    Machine learning new electrcity  \n",
      "3  There would less hype around AI action going f...  \n",
      "4                                   python best tool  \n",
      "5                                    R good language  \n",
      "6                                        I like Book  \n",
      "7                            I want books like this.  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['tweet_without_stopwords'] =df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed23754",
   "metadata": {
    "papermill": {
     "duration": 0.039803,
     "end_time": "2022-05-18T13:42:38.943677",
     "exception": false,
     "start_time": "2022-05-18T13:42:38.903874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Standardizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860ca62",
   "metadata": {
    "papermill": {
     "duration": 0.039329,
     "end_time": "2022-05-18T13:42:39.023132",
     "exception": false,
     "start_time": "2022-05-18T13:42:38.983803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " we are going to discuss how to standardize the text. But before\n",
    "that, let‚Äôs understand what is text standardization and why we need to do it.\n",
    "Most of the text data is in the form of either customer reviews, blogs, or tweets,\n",
    "where there is a high chance of people using short words and abbreviations to\n",
    "represent the same meaning. This may help the downstream process to easily\n",
    "understand and resolve the semantics of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769eb6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:39.104880Z",
     "iopub.status.busy": "2022-05-18T13:42:39.104317Z",
     "iopub.status.idle": "2022-05-18T13:42:39.107974Z",
     "shell.execute_reply": "2022-05-18T13:42:39.107341Z"
    },
    "papermill": {
     "duration": 0.046131,
     "end_time": "2022-05-18T13:42:39.109738",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.063607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lookup_dict = {'nlp':'Natural language processing',\n",
    "'ur':'your', 'wbu': 'what about you'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7087a5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:39.191383Z",
     "iopub.status.busy": "2022-05-18T13:42:39.190858Z",
     "iopub.status.idle": "2022-05-18T13:42:39.196285Z",
     "shell.execute_reply": "2022-05-18T13:42:39.195510Z"
    },
    "papermill": {
     "duration": 0.048982,
     "end_time": "2022-05-18T13:42:39.198260",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.149278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_std(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = re.sub(r'[^\\w\\s]',' ',word)\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "            new_words.append(word)\n",
    "            new_text = \" \".join(new_words)\n",
    "            return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cab26a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:39.278655Z",
     "iopub.status.busy": "2022-05-18T13:42:39.278143Z",
     "iopub.status.idle": "2022-05-18T13:42:39.283168Z",
     "shell.execute_reply": "2022-05-18T13:42:39.282322Z"
    },
    "papermill": {
     "duration": 0.047427,
     "end_time": "2022-05-18T13:42:39.284971",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.237544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what about you'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_std('I like wbu its nlp choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42caab",
   "metadata": {
    "papermill": {
     "duration": 0.039609,
     "end_time": "2022-05-18T13:42:39.364508",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.324899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Correcting Spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7838653",
   "metadata": {
    "papermill": {
     "duration": 0.040018,
     "end_time": "2022-05-18T13:42:39.444397",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.404379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This will help us in reducing multiple copies of words,\n",
    "which represents the same meaning. For example, ‚Äúproccessing‚Äù and\n",
    "‚Äúprocessing‚Äù will be treated as different words even if they are used in the\n",
    "same sense.\n",
    "Note that abbreviations should be handled before this step, or else\n",
    "the corrector would fail at times. Say, for example, ‚Äúur‚Äù (actually means\n",
    "‚Äúyour‚Äù) would be corrected to ‚Äúor.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57b0be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:39.525702Z",
     "iopub.status.busy": "2022-05-18T13:42:39.525180Z",
     "iopub.status.idle": "2022-05-18T13:42:39.528957Z",
     "shell.execute_reply": "2022-05-18T13:42:39.528380Z"
    },
    "papermill": {
     "duration": 0.04642,
     "end_time": "2022-05-18T13:42:39.530675",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.484255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = ['This will help us in reducing multiple copies of words',\n",
    "       'which represents the same meaning','For example, ‚Äúproccessing‚Äù and ‚Äúprocessing‚Äù',\n",
    "       'will be treated as different words even if they are used in the same sense.',\n",
    "       'Note that abbreviations should be handled before this step, or else',\n",
    "        'the corrector would fail at times.',\n",
    "       'Say, for example, ‚Äúur‚Äù (actually means‚Äúyour‚Äù) would be corrected to ‚Äúor.‚Äù']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc6556a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:39.612061Z",
     "iopub.status.busy": "2022-05-18T13:42:39.611520Z",
     "iopub.status.idle": "2022-05-18T13:42:39.619891Z",
     "shell.execute_reply": "2022-05-18T13:42:39.619280Z"
    },
    "papermill": {
     "duration": 0.051231,
     "end_time": "2022-05-18T13:42:39.621622",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.570391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This will help us in reducing multiple copies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>which represents the same meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For example, ‚Äúproccessing‚Äù and ‚Äúprocessing‚Äù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>will be treated as different words even if the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Note that abbreviations should be handled befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the corrector would fail at times.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Say, for example, ‚Äúur‚Äù (actually means‚Äúyour‚Äù) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  This will help us in reducing multiple copies ...\n",
       "1                  which represents the same meaning\n",
       "2        For example, ‚Äúproccessing‚Äù and ‚Äúprocessing‚Äù\n",
       "3  will be treated as different words even if the...\n",
       "4  Note that abbreviations should be handled befo...\n",
       "5                 the corrector would fail at times.\n",
       "6  Say, for example, ‚Äúur‚Äù (actually means‚Äúyour‚Äù) ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert list to DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet': txt})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f48f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:39.704653Z",
     "iopub.status.busy": "2022-05-18T13:42:39.704153Z",
     "iopub.status.idle": "2022-05-18T13:42:40.225408Z",
     "shell.execute_reply": "2022-05-18T13:42:40.224521Z"
    },
    "papermill": {
     "duration": 0.565835,
     "end_time": "2022-05-18T13:42:40.227716",
     "exception": false,
     "start_time": "2022-05-18T13:42:39.661881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    His will help us in reducing multiple copies o...\n",
       "1                    which represents the same meaning\n",
       "2           For example, ‚Äúprocessing‚Äù and ‚Äúprocessing‚Äù\n",
       "3    will be treated as different words even if the...\n",
       "4    Note that abbreviations should be handled befo...\n",
       "5                     the correct would fail at times.\n",
       "6    May, for example, ‚Äúor‚Äù (actually means‚Äúyour‚Äù) ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "df['tweet']= df['tweet'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "df['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc5ff9",
   "metadata": {
    "papermill": {
     "duration": 0.040372,
     "end_time": "2022-05-18T13:42:40.309373",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.269001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880b294",
   "metadata": {
    "papermill": {
     "duration": 0.04047,
     "end_time": "2022-05-18T13:42:40.390560",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.350090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tokenization refers to splitting text into minimal meaningful units. There is a sentence tokenizer\n",
    "and word tokenizer. We will see a word tokenizer in this recipe, which is\n",
    "a mandatory step in text preprocessing for any kind of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f88f8dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:40.473717Z",
     "iopub.status.busy": "2022-05-18T13:42:40.473409Z",
     "iopub.status.idle": "2022-05-18T13:42:40.501319Z",
     "shell.execute_reply": "2022-05-18T13:42:40.500301Z"
    },
    "papermill": {
     "duration": 0.072424,
     "end_time": "2022-05-18T13:42:40.503536",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.431112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['His',\n",
       " 'will',\n",
       " 'help',\n",
       " 'us',\n",
       " 'in',\n",
       " 'reducing',\n",
       " 'multiple',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'words']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.word_tokenize(df[\"tweet\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1cb289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:40.587064Z",
     "iopub.status.busy": "2022-05-18T13:42:40.586774Z",
     "iopub.status.idle": "2022-05-18T13:42:40.602741Z",
     "shell.execute_reply": "2022-05-18T13:42:40.602202Z"
    },
    "papermill": {
     "duration": 0.060394,
     "end_time": "2022-05-18T13:42:40.604981",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.544587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This will help us in reducing multiple copies ...</td>\n",
       "      <td>[This, will, help, us, in, reducing, multiple,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>which represents the same meaning</td>\n",
       "      <td>[which, represents, the, same, meaning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For example, ‚Äúproccessing‚Äù and ‚Äúprocessing‚Äù</td>\n",
       "      <td>[For, example, ,, ‚Äú, proccessing, ‚Äù, and, ‚Äú, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>will be treated as different words even if the...</td>\n",
       "      <td>[will, be, treated, as, different, words, even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Note that abbreviations should be handled befo...</td>\n",
       "      <td>[Note, that, abbreviations, should, be, handle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the corrector would fail at times.</td>\n",
       "      <td>[the, corrector, would, fail, at, times, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Say, for example, ‚Äúur‚Äù (actually means‚Äúyour‚Äù) ...</td>\n",
       "      <td>[Say, ,, for, example, ,, ‚Äú, ur, ‚Äù, (, actuall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  This will help us in reducing multiple copies ...   \n",
       "1                  which represents the same meaning   \n",
       "2        For example, ‚Äúproccessing‚Äù and ‚Äúprocessing‚Äù   \n",
       "3  will be treated as different words even if the...   \n",
       "4  Note that abbreviations should be handled befo...   \n",
       "5                 the corrector would fail at times.   \n",
       "6  Say, for example, ‚Äúur‚Äù (actually means‚Äúyour‚Äù) ...   \n",
       "\n",
       "                                     tokenized_sents  \n",
       "0  [This, will, help, us, in, reducing, multiple,...  \n",
       "1            [which, represents, the, same, meaning]  \n",
       "2  [For, example, ,, ‚Äú, proccessing, ‚Äù, and, ‚Äú, p...  \n",
       "3  [will, be, treated, as, different, words, even...  \n",
       "4  [Note, that, abbreviations, should, be, handle...  \n",
       "5        [the, corrector, would, fail, at, times, .]  \n",
       "6  [Say, ,, for, example, ,, ‚Äú, ur, ‚Äù, (, actuall...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#convert list to DataFrame\n",
    "df = pd.DataFrame({'tweet': txt})\n",
    "df['tokenized_sents'] = df.apply(lambda x: nltk.word_tokenize(x['tweet']), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b22075",
   "metadata": {
    "papermill": {
     "duration": 0.042497,
     "end_time": "2022-05-18T13:42:40.688996",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.646499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1169597",
   "metadata": {
    "papermill": {
     "duration": 0.041468,
     "end_time": "2022-05-18T13:42:40.771925",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.730457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "we will discuss stemming. Stemming is a process of\n",
    "extracting a root word. For example, ‚Äúfish,‚Äù ‚Äúfishes,‚Äù and ‚Äúfishing‚Äù are stemmed into fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "880c3cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:40.857756Z",
     "iopub.status.busy": "2022-05-18T13:42:40.856849Z",
     "iopub.status.idle": "2022-05-18T13:42:40.863842Z",
     "shell.execute_reply": "2022-05-18T13:42:40.863220Z"
    },
    "papermill": {
     "duration": 0.052339,
     "end_time": "2022-05-18T13:42:40.865932",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.813593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            tweet\n",
      "0                  I like fishing\n",
      "1                I am eating fish\n",
      "2  There are many fishes in pound\n"
     ]
    }
   ],
   "source": [
    "text=['I like fishing','I am eating fish','There are many fishes in pound']\n",
    "#convert list to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet':text})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fd81786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:40.951718Z",
     "iopub.status.busy": "2022-05-18T13:42:40.951202Z",
     "iopub.status.idle": "2022-05-18T13:42:40.963411Z",
     "shell.execute_reply": "2022-05-18T13:42:40.962595Z"
    },
    "papermill": {
     "duration": 0.057698,
     "end_time": "2022-05-18T13:42:40.965561",
     "exception": false,
     "start_time": "2022-05-18T13:42:40.907863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like fishing</td>\n",
       "      <td>I like fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am eating fish</td>\n",
       "      <td>I am eat fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are many fishes in pound</td>\n",
       "      <td>there are mani fish in pound</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            tweet                tweet_stemming\n",
       "0                  I like fishing                   I like fish\n",
       "1                I am eating fish                 I am eat fish\n",
       "2  There are many fishes in pound  there are mani fish in pound"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import library\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "df['tweet_stemming'] = df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for\n",
    "word in x.split()]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb20c1",
   "metadata": {
    "papermill": {
     "duration": 0.043765,
     "end_time": "2022-05-18T13:42:41.052688",
     "exception": false,
     "start_time": "2022-05-18T13:42:41.008923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3420f54",
   "metadata": {
    "papermill": {
     "duration": 0.042778,
     "end_time": "2022-05-18T13:42:41.138294",
     "exception": false,
     "start_time": "2022-05-18T13:42:41.095516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lemmatization is a process of\n",
    "extracting a root word by considering the vocabulary. For example, ‚Äúgood,‚Äù\n",
    "‚Äúbetter,‚Äù or ‚Äúbest‚Äù is lemmatized into good.\n",
    "The part of speech of a word is determined in lemmatization. It will\n",
    "return the dictionary form of a word, which must be a valid word while\n",
    "stemming just extracts the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a789208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:41.225113Z",
     "iopub.status.busy": "2022-05-18T13:42:41.224809Z",
     "iopub.status.idle": "2022-05-18T13:42:41.232061Z",
     "shell.execute_reply": "2022-05-18T13:42:41.231225Z"
    },
    "papermill": {
     "duration": 0.053307,
     "end_time": "2022-05-18T13:42:41.234012",
     "exception": false,
     "start_time": "2022-05-18T13:42:41.180705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            tweet\n",
      "0                  I like fishing\n",
      "1                   I eating fish\n",
      "2  There are many fishes in pound\n",
      "3                 leaves and leaf\n"
     ]
    }
   ],
   "source": [
    "text=['I like fishing','I eating fish','There are many fishes in pound', 'leaves and leaf']\n",
    "#convert list to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tweet':text})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4bd4ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:42:41.321751Z",
     "iopub.status.busy": "2022-05-18T13:42:41.321456Z",
     "iopub.status.idle": "2022-05-18T13:42:43.458032Z",
     "shell.execute_reply": "2022-05-18T13:42:43.457082Z"
    },
    "papermill": {
     "duration": 2.182916,
     "end_time": "2022-05-18T13:42:43.460200",
     "exception": false,
     "start_time": "2022-05-18T13:42:41.277284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  I like fishing\n",
       "1                   I eating fish\n",
       "2    There are many fish in pound\n",
       "3                   leaf and leaf\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import library\n",
    "from textblob import Word\n",
    "#Code for lemmatize\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "df['tweet']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.778047,
   "end_time": "2022-05-18T13:42:44.432009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-18T13:42:26.653962",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
